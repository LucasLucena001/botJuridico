#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot Jur√≠dico - Execu√ß√£o Local Otimizada
Projeto Final - P√≥s-Gradua√ß√£o iCEV - NLP

Script principal para execu√ß√£o local com configura√ß√µes otimizadas.
"""

import os
import sys
import time
import warnings
from pathlib import Path

# Configura√ß√µes para melhor performance
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
warnings.filterwarnings('ignore')

def check_requirements():
    """Verifica depend√™ncias essenciais."""
    required = ['torch', 'transformers', 'sentence_transformers', 'faiss', 'gradio', 'sklearn']
    missing = []
    
    for pkg in required:
        try:
            if pkg == 'faiss':
                import faiss
            elif pkg == 'sklearn':
                import sklearn
            else:
                __import__(pkg)
        except ImportError:
            missing.append(pkg)
    
    if missing:
        print(f"‚ùå Instale: pip install {' '.join(missing)}")
        return False
    return True

def setup_gpu():
    """Configura GPU se dispon√≠vel."""
    try:
        import torch
        if torch.cuda.is_available():
            device = torch.cuda.get_device_name(0)
            memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"üéÆ GPU: {device} ({memory:.1f}GB)")
            torch.cuda.empty_cache()
            return 'cuda'
        else:
            print("üíª Usando CPU")
            return 'cpu'
    except:
        print("üíª PyTorch n√£o encontrado, usando CPU")
        return 'cpu'

class LocalChatbot:
    """Chatbot otimizado para execu√ß√£o local."""
    
    def __init__(self, data_dir="dados"):
        print("üöÄ Inicializando Chatbot Local...")
        
        self.device = setup_gpu()
        self.data_dir = Path(data_dir)
        self.documents = {}
        self.chunks = []
        self.embeddings_model = None
        self.vectors = None
        self.index = None
        
        self._load_system()
    
    def _load_system(self):
        """Carrega sistema completo."""
        try:
            # 1. Carregar documentos
            self._load_documents()
            
            # 2. Processar chunks
            self._create_chunks()
            
            # 3. Carregar modelo de embeddings
            self._load_embeddings_model()
            
            # 4. Gerar embeddings
            self._generate_embeddings()
            
            # 5. Criar √≠ndice de busca
            self._create_search_index()
            
            print("‚úÖ Sistema carregado com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            raise
    
    def _load_documents(self):
        """Carrega documentos jur√≠dicos."""
        print("üìö Carregando documentos...")
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Pasta '{self.data_dir}' n√£o encontrada!")
        
        txt_files = list(self.data_dir.glob("*.txt"))
        if not txt_files:
            raise FileNotFoundError("Nenhum arquivo .txt encontrado!")
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if len(content) > 100:
                    self.documents[txt_file.stem] = self._clean_text(content)
                    print(f"‚úÖ {txt_file.stem}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erro em {txt_file.name}: {e}")
        
        print(f"üìö {len(self.documents)} documentos carregados")
    
    def _clean_text(self, text):
        """Limpeza b√°sica de texto."""
        import re
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 10]
        return '\n'.join(lines)
    
    def _create_chunks(self):
        """Cria chunks dos documentos."""
        print("üî™ Criando chunks...")
        
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=800,
                chunk_overlap=150,
                length_function=len
            )
            
            for doc_name, content in self.documents.items():
                chunks = splitter.split_text(content)
                
                for i, chunk_text in enumerate(chunks):
                    self.chunks.append({
                        'text': chunk_text,
                        'doc_name': doc_name,
                        'chunk_id': i
                    })
            
            print(f"üß© {len(self.chunks)} chunks criados")
            
        except ImportError:
            # Fallback simples se langchain n√£o estiver dispon√≠vel
            print("‚ö†Ô∏è LangChain n√£o dispon√≠vel, usando chunking simples")
            
            for doc_name, content in self.documents.items():
                words = content.split()
                chunk_size = 200
                
                for i in range(0, len(words), chunk_size):
                    chunk_words = words[i:i + chunk_size]
                    chunk_text = ' '.join(chunk_words)
                    
                    self.chunks.append({
                        'text': chunk_text,
                        'doc_name': doc_name,
                        'chunk_id': i // chunk_size
                    })
            
            print(f"üß© {len(self.chunks)} chunks criados (m√©todo simples)")
    
    def _load_embeddings_model(self):
        """Carrega modelo de embeddings."""
        print("ü§ñ Carregando modelo de embeddings...")
        
        from sentence_transformers import SentenceTransformer
        
        # Modelo leve e eficiente
        model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        
        self.embeddings_model = SentenceTransformer(
            model_name,
            device=self.device
        )
        
        print("‚úÖ Modelo carregado")
    
    def _generate_embeddings(self):
        """Gera embeddings dos chunks."""
        print("üß† Gerando embeddings...")
        
        texts = [chunk['text'] for chunk in self.chunks]
        
        # Batch size baseado no dispositivo
        batch_size = 32 if self.device == 'cuda' else 16
        
        self.vectors = self.embeddings_model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        print(f"‚úÖ Embeddings gerados: {self.vectors.shape}")
    
    def _create_search_index(self):
        """Cria √≠ndice de busca FAISS."""
        print("üìä Criando √≠ndice de busca...")
        
        try:
            import faiss
            import numpy as np
            
            # Garantir tipo float32
            if self.vectors.dtype != np.float32:
                self.vectors = self.vectors.astype(np.float32)
            
            # Criar √≠ndice
            dimension = self.vectors.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)
            self.index.add(self.vectors)
            
            print(f"‚úÖ √çndice criado: {len(self.chunks)} vetores")
            
        except ImportError:
            print("‚ö†Ô∏è FAISS n√£o dispon√≠vel, usando busca linear")
            self.index = None
    
    def search(self, query, k=5):
        """Busca documentos similares."""
        # Gerar embedding da query
        query_vector = self.embeddings_model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        if self.index is not None:
            # Busca com FAISS
            import faiss
            import numpy as np
            
            if query_vector.dtype != np.float32:
                query_vector = query_vector.astype(np.float32)
            
            scores, indices = self.index.search(query_vector, k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0:
                    chunk = self.chunks[idx].copy()
                    chunk['similarity_score'] = float(score)
                    results.append(chunk)
            
            return results
        
        else:
            # Busca linear
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_vector, self.vectors)[0]
            top_indices = np.argsort(similarities)[-k:][::-1]
            
            results = []
            for idx in top_indices:
                chunk = self.chunks[idx].copy()
                chunk['similarity_score'] = float(similarities[idx])
                results.append(chunk)
            
            return results
    
    def classify_query(self, query):
        """Classifica tipo de query."""
        query_lower = query.lower()
        
        # Keywords jur√≠dicas
        legal_keywords = [
            'artigo', 'lei', 'c√≥digo', 'direito', 'constitui√ß√£o',
            'clt', 'cdc', 'prazo', 'crime', 'pena', 'processo',
            'habeas', 'mandado', 'a√ß√£o', 'recurso'
        ]
        
        # Keywords conversacionais
        chat_keywords = [
            'ol√°', 'oi', 'bom dia', 'boa tarde', 'boa noite',
            'obrigado', 'obrigada', 'tchau', 'at√© logo',
            'como voc√™', 'quem √©', 'funciona'
        ]
        
        if any(kw in query_lower for kw in legal_keywords):
            return 'juridica'
        elif any(kw in query_lower for kw in chat_keywords):
            return 'conversa'
        else:
            return 'fora_escopo'
    
    def generate_answer(self, query, results):
        """Gera resposta baseada nos resultados."""
        if not results:
            return "N√£o encontrei informa√ß√µes relevantes na base de conhecimento."
        
        # Pegar melhor resultado
        best_result = results[0]
        doc_name = best_result['doc_name'].replace('_', ' ')
        text = best_result['text']
        score = best_result['similarity_score']
        
        # Resposta baseada em template
        answer = f"Com base na legisla√ß√£o brasileira:\n\n"
        answer += f"**Fonte:** {doc_name}\n"
        answer += f"**Relev√¢ncia:** {score:.3f}\n\n"
        
        # Limitar tamanho do texto
        if len(text) > 500:
            text = text[:500] + "..."
        
        answer += text
        
        if len(results) > 1:
            answer += f"\n\n*Encontrei tamb√©m informa√ß√µes em {len(results)-1} outro(s) documento(s).*"
        
        answer += "\n\n‚öñÔ∏è **Aviso:** Esta resposta √© informativa. Para casos espec√≠ficos, consulte um advogado."
        
        return answer
    
    def query(self, question):
        """Processa uma consulta completa."""
        start_time = time.time()
        
        # Classificar query
        classification = self.classify_query(question)
        
        if classification == 'juridica':
            # Buscar contexto
            results = self.search(question, k=3)
            answer = self.generate_answer(question, results)
            sources = [r['doc_name'] for r in results]
            
        elif classification == 'conversa':
            # Resposta conversacional
            if any(word in question.lower() for word in ['ol√°', 'oi', 'bom dia']):
                answer = "Ol√°! Sou seu assistente jur√≠dico. Como posso ajudar com quest√µes legais?"
            elif any(word in question.lower() for word in ['obrigado', 'valeu']):
                answer = "De nada! Estou aqui para ajudar com d√∫vidas jur√≠dicas."
            elif any(word in question.lower() for word in ['como funciona', 'quem √©']):
                answer = "Sou um chatbot especializado em legisla√ß√£o brasileira. Posso consultar leis, c√≥digos e normas para responder suas d√∫vidas."
            else:
                answer = "Ol√°! Como posso ajudar com quest√µes jur√≠dicas hoje?"
            
            results = []
            sources = []
            
        else:  # fora_escopo
            answer = "Desculpe, minha especialidade √© legisla√ß√£o brasileira. Posso ajudar com consultas sobre leis, c√≥digos e normas jur√≠dicas."
            results = []
            sources = []
        
        processing_time = time.time() - start_time
        
        return {
            'question': question,
            'answer': answer,
            'classification': classification,
            'sources': sources,
            'processing_time': processing_time,
            'results_count': len(results) if results else 0
        }

def create_gradio_interface(chatbot):
    """Cria interface Gradio."""
    try:
        import gradio as gr
        
        def chat_function(message, history):
            if not message.strip():
                return history, ""
            
            # Processar consulta
            response = chatbot.query(message)
            
            # Formatar resposta
            answer = response['answer']
            footer = f"\n\n---\nüìä Tempo: {response['processing_time']:.2f}s"
            footer += f" | üè∑Ô∏è Tipo: {response['classification']}"
            
            if response['sources']:
                footer += f" | üìö Fontes: {response['results_count']}"
            
            full_response = answer + footer
            
            # Atualizar hist√≥rico
            history.append([message, full_response])
            
            return history, ""
        
        # Interface
        with gr.Blocks(
            title="Chatbot Jur√≠dico Local",
            theme=gr.themes.Soft()
        ) as interface:
            
            gr.Markdown("""
            # ‚öñÔ∏è Chatbot Jur√≠dico Inteligente
            **Sistema RAG para Consultas Jur√≠dicas Brasileiras**
            """)
            
            chatbot_ui = gr.Chatbot(
                height=500,
                show_label=False
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    placeholder="Digite sua pergunta jur√≠dica...",
                    label="Mensagem",
                    scale=4
                )
                send_btn = gr.Button("Enviar", variant="primary", scale=1)
            
            # Exemplos
            gr.Examples(
                examples=[
                    "O que diz o artigo 5¬∫ da Constitui√ß√£o Federal?",
                    "Qual o prazo para reclamar de v√≠cios em produtos dur√°veis?",
                    "Como funciona a jornada de trabalho na CLT?",
                    "O que √© considerado uni√£o est√°vel?",
                    "Ol√°, como voc√™ funciona?"
                ],
                inputs=msg,
                label="üí° Exemplos de perguntas:"
            )
            
            # Eventos
            msg.submit(chat_function, [msg, chatbot_ui], [chatbot_ui, msg])
            send_btn.click(chat_function, [msg, chatbot_ui], [chatbot_ui, msg])
        
        return interface
        
    except ImportError:
        print("‚ùå Gradio n√£o dispon√≠vel")
        return None

def run_cli_interface(chatbot):
    """Interface de linha de comando."""
    print("\n" + "="*60)
    print("üí¨ INTERFACE DE LINHA DE COMANDO")
    print("="*60)
    print("Digite 'sair' para encerrar")
    print("="*60)
    
    while True:
        try:
            question = input("\nüë§ Voc√™: ").strip()
            
            if question.lower() in ['sair', 'quit', 'exit']:
                print("üëã At√© logo!")
                break
            
            if not question:
                continue
            
            print("ü§î Processando...")
            response = chatbot.query(question)
            
            print(f"\nü§ñ Assistente:")
            print(response['answer'])
            
            print(f"\nüìä Info: {response['classification']} | {response['processing_time']:.2f}s")
            if response['sources']:
                print(f"üìö Fontes: {', '.join(response['sources'][:3])}")
            
        except KeyboardInterrupt:
            print("\nüëã At√© logo!")
            break
        except Exception as e:
            print(f"\n‚ùå Erro: {e}")

def main():
    """Fun√ß√£o principal."""
    print("üöÄ CHATBOT JUR√çDICO - EXECU√á√ÉO LOCAL")
    print("="*50)
    
    # Verificar depend√™ncias
    if not check_requirements():
        return
    
    # Verificar dados
    if not Path("dados").exists():
        print("‚ùå Pasta 'dados' n√£o encontrada!")
        print("üí° Crie a pasta 'dados' e adicione os arquivos .txt da legisla√ß√£o")
        return
    
    try:
        # Inicializar chatbot
        chatbot = LocalChatbot()
        
        # Escolher interface
        print("\nüéØ Escolha a interface:")
        print("1. Interface Web (Gradio) - Recomendado")
        print("2. Linha de Comando (CLI)")
        
        choice = input("\nEscolha (1 ou 2): ").strip()
        
        if choice == "1":
            print("\nüåê Iniciando interface web...")
            interface = create_gradio_interface(chatbot)
            
            if interface:
                interface.launch(
                    server_name="0.0.0.0",
                    server_port=7860,
                    share=False,
                    show_error=True
                )
            else:
                print("‚ùå Gradio n√£o dispon√≠vel, usando CLI...")
                run_cli_interface(chatbot)
        
        else:
            run_cli_interface(chatbot)
    
    except Exception as e:
        print(f"‚ùå Erro: {e}")

if __name__ == "__main__":
    main()